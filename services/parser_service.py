# å¤šæ¨¡æ€è§£ææœåŠ¡
import os
import io
import base64
from typing import Dict, Any, Optional, List
from fastapi import UploadFile, HTTPException
import PyPDF2
import docx
from PIL import Image
import pytesseract
from pdf2image import convert_from_bytes
import requests
from config import settings
from services.embedding_service import embedding_service
from services.database_service import database_service

class ParserService:
    """
    å¤šæ¨¡æ€è§£ææœåŠ¡ - å¤„ç†æ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ã€æ–‡æ¡£ç­‰ä¸åŒæ ¼å¼çš„æ–‡ä»¶
    """
    
    def __init__(self):
        self.supported_types = {
            'pdf': self._parse_pdf,
            'docx': self._parse_docx,
            'txt': self._parse_text,
            'md': self._parse_text,
            'png': self._parse_image,
            'jpg': self._parse_image,
            'jpeg': self._parse_image,
            'gif': self._parse_image
        }
        self.max_tokens = 6000  # è®¾ç½®æœ€å¤§tokenæ•°ï¼Œç•™ä¸€äº›ä½™é‡
    
    def _split_text_into_chunks(self, text: str, max_tokens: int = None) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå—"""
        if max_tokens is None:
            max_tokens = self.max_tokens
        
        # æ›´ä¿å®ˆçš„tokenä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦1.2å€ï¼Œè‹±æ–‡å•è¯1.5å€
        def estimate_tokens(text: str) -> int:
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            english_words = len(text.split())
            # æ›´ä¿å®ˆçš„ä¼°ç®—
            return int(chinese_chars * 1.2 + english_words * 1.5)
        
        if estimate_tokens(text) <= max_tokens:
            return [text]
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if estimate_tokens(paragraph) > max_tokens:
                # å…ˆä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # åˆ†å‰²è¿‡é•¿çš„æ®µè½
                sub_chunks = self._split_long_paragraph(paragraph, max_tokens)
                chunks.extend(sub_chunks)
            elif estimate_tokens(current_chunk + paragraph) <= max_tokens:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_long_paragraph(self, paragraph: str, max_tokens: int) -> List[str]:
        """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
        def estimate_tokens(text: str) -> int:
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            english_words = len(text.split())
            return int(chinese_chars * 1.2 + english_words * 1.5)
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = paragraph.split('ã€‚')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            if estimate_tokens(current_chunk + sentence) <= max_tokens:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    async def parse_file(self, file: UploadFile, s3_data: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """
        è§£æä¸Šä¼ çš„æ–‡ä»¶å¹¶åˆ›å»ºè®°å¿†å•å…ƒ
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶
            s3_data: S3ä¸Šä¼ è¿”å›çš„æ•°æ®
            user_id: ç”¨æˆ·ID
            
        Returns:
            Dict: è§£æç»“æœå’Œè®°å¿†ID
        """
        try:
            file_extension = file.filename.split('.')[-1].lower()
            
            if file_extension not in self.supported_types:
                raise HTTPException(
                    status_code=400,
                    detail=f"Unsupported file type: {file_extension}"
                )
            
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            await file.seek(0)
            file_content = await file.read()
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è§£æ
            parser_func = self.supported_types[file_extension]
            parsed_content = await parser_func(file_content, file)
            
            # ç”Ÿæˆembedding
            embedding = embedding_service.generate_embedding(
                text=parsed_content['text'],
                input_type="passage"
            )
            
            # åˆ›å»ºè®°å¿†å•å…ƒ
            memory_id = database_service.create_memory(
                content=parsed_content['text'],
                memory_type=parsed_content['type'],
                embedding=embedding,
                user_id=user_id,
                metadata={
                    'original_filename': file.filename,
                    'file_size': len(file_content),
                    'file_extension': file_extension,
                    's3_key': s3_data.get('s3_key'),
                    's3_url': s3_data.get('file_url'),
                    **parsed_content.get('metadata', {})
                },
                source=s3_data.get('file_url'),
                summary=parsed_content.get('summary'),
                tags=parsed_content.get('tags', [])
            )
            # å¤„ç†é¢å¤–çš„æ–‡æœ¬å—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            additional_memories = []
            if 'additional_chunks' in parsed_content:
                for i, chunk in enumerate(parsed_content['additional_chunks']):
                    try:
                        # ä¸ºæ¯ä¸ªå—ç”Ÿæˆembedding
                        chunk_embedding = embedding_service.generate_embedding(
                            text=chunk,
                            input_type="passage"
                        )
                        
                        # âœ… åˆ›å»ºé¢å¤–çš„è®°å¿†å•å…ƒ - æ·»åŠ  user_id å‚æ•°
                        chunk_memory_id = database_service.create_memory(
                            content=chunk,
                            memory_type=parsed_content['type'],
                            embedding=chunk_embedding,
                            user_id=user_id,  # âœ… æ·»åŠ  user_id å‚æ•°
                            metadata={
                                'original_filename': file.filename,
                                'file_size': len(file_content),
                                'file_extension': file_extension,
                                's3_key': s3_data.get('s3_key'),
                                's3_url': s3_data.get('file_url'),
                                'chunk_index': i + 1,
                                'total_chunks': parsed_content['metadata'].get('total_chunks', 1),
                                'is_partial': True,
                                **parsed_content.get('metadata', {})
                            },
                            source=s3_data.get('file_url'),
                            summary=chunk[:200] + "..." if len(chunk) > 200 else chunk,
                            tags=parsed_content.get('tags', [])
                        )
                        
                        additional_memories.append({
                            'memory_id': chunk_memory_id,
                            'chunk_index': i + 1,
                            'content_preview': chunk[:100] + "..." if len(chunk) > 100 else chunk
                        })
                        
                    except Exception as e:
                        print(f"âš ï¸  Failed to process chunk {i + 1}: {e}")
                        import traceback
                        traceback.print_exc()
                        continue
            
            return {
                'success': True,
                'memory_id': memory_id,
                'parsed_content': parsed_content,
                'embedding_dimension': len(embedding),
                'additional_memories': additional_memories
            }
            
        except Exception as e:
            print(f"âŒ File parsing failed: {e}")
            raise HTTPException(status_code=500, detail=f"Parsing failed: {str(e)}")
    
    async def _parse_pdf(self, content: bytes, file: UploadFile) -> Dict[str, Any]:
        """è§£æPDFæ–‡ä»¶"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
            text = ""
            
            # é¦–å…ˆå°è¯•ç›´æ¥æå–æ–‡æœ¬
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                page_text = page.extract_text()
                text += page_text + "\n"
            
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            
            # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–å¾ˆçŸ­ï¼Œå°è¯•OCR
            if not text or len(text) < 50:
                print("ğŸ“„ PDFæ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•OCRè¯†åˆ«...")
                try:
                    # å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡
                    images = convert_from_bytes(content)
                    ocr_text = ""
                    
                    for i, image in enumerate(images):
                        print(f"ğŸ” OCRå¤„ç†ç¬¬{i+1}é¡µ...")
                        # ä½¿ç”¨pytesseractè¿›è¡ŒOCR
                        page_text = pytesseract.image_to_string(image, lang='chi_sim+eng')
                        ocr_text += page_text + "\n"
                    
                    if ocr_text.strip():
                        text = ocr_text.strip()
                        print(f"âœ… OCRæˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(text)}")
                    else:
                        print("âš ï¸ OCRä¹Ÿæœªæå–åˆ°æ–‡æœ¬")
                        
                except Exception as ocr_error:
                    print(f"âŒ OCRå¤±è´¥: {ocr_error}")
                    # OCRå¤±è´¥æ—¶ï¼Œè¿”å›æç¤ºä¿¡æ¯è€Œä¸æ˜¯ç©ºæ–‡æœ¬
                    text = f"[æ‰«æç‰ˆPDF - éœ€è¦OCRè¯†åˆ«] æ­¤PDFæ–‡ä»¶æ˜¯æ‰«æç‰ˆï¼Œæ— æ³•ç›´æ¥æå–æ–‡æœ¬ã€‚è¯·å®‰è£…OCRä¾èµ–æˆ–ä½¿ç”¨å¯ç¼–è¾‘çš„PDFæ–‡ä»¶ã€‚é”™è¯¯è¯¦æƒ…: {str(ocr_error)}"
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™åˆ†å—
            text_chunks = self._split_text_into_chunks(text)
            
            if len(text_chunks) > 1:
                # å¦‚æœæ–‡æœ¬è¢«åˆ†å—ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå—ï¼Œå…¶ä»–å—ä½œä¸ºé¢å¤–ä¿¡æ¯
                return {
                    'text': text_chunks[0],
                    'type': 'document',
                    'metadata': {
                        'page_count': len(pdf_reader.pages),
                        'parser': 'PyPDF2+OCR' if len(text) > 50 else 'PyPDF2',
                        'total_chunks': len(text_chunks),
                        'chunk_index': 0,
                        'is_partial': True
                    },
                    'summary': text[:200] + "..." if len(text) > 200 else text,
                    'tags': ['pdf', 'document'],
                    'additional_chunks': text_chunks[1:]  # å…¶ä»–å—
                }
            else:
                return {
                    'text': text,
                    'type': 'document',
                    'metadata': {
                        'page_count': len(pdf_reader.pages),
                        'parser': 'PyPDF2+OCR' if len(text) > 50 else 'PyPDF2'
                    },
                    'summary': text[:200] + "..." if len(text) > 200 else text,
                    'tags': ['pdf', 'document']
                }
            
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"PDF parsing failed: {str(e)}")
    
    async def _parse_docx(self, content: bytes, file: UploadFile) -> Dict[str, Any]:
        """è§£æDOCXæ–‡ä»¶"""
        try:
            doc = docx.Document(io.BytesIO(content))
            text = ""
            
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            
            return {
                'text': text,
                'type': 'document',
                'metadata': {
                    'paragraph_count': len(doc.paragraphs),
                    'parser': 'python-docx'
                },
                'summary': text[:200] + "..." if len(text) > 200 else text,
                'tags': ['docx', 'document']
            }
            
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"DOCX parsing failed: {str(e)}")
    
    async def _parse_text(self, content: bytes, file: UploadFile) -> Dict[str, Any]:
        """è§£ææ–‡æœ¬æ–‡ä»¶"""
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            text = ""
            
            for encoding in encodings:
                try:
                    text = content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if not text:
                text = content.decode('utf-8', errors='ignore')
            
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            
            return {
                'text': text,
                'type': 'text',
                'metadata': {
                    'encoding': 'utf-8',
                    'parser': 'builtin'
                },
                'summary': text[:200] + "..." if len(text) > 200 else text,
                'tags': ['text', 'plain']
            }
            
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Text parsing failed: {str(e)}")
    
    async def _parse_image(self, content: bytes, file: UploadFile) -> Dict[str, Any]:
        """è§£æå›¾ç‰‡æ–‡ä»¶"""
        try:
            # æ‰“å¼€å›¾ç‰‡
            image = Image.open(io.BytesIO(content))
            
            # è·å–å›¾ç‰‡ä¿¡æ¯
            width, height = image.size
            format_name = image.format
            mode = image.mode
            
            # ç”Ÿæˆå›¾ç‰‡æè¿°ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨CLIPæˆ–OCRï¼‰
            image_description = f"Image: {width}x{height} pixels, format: {format_name}, mode: {mode}"
            
            # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç”¨äºå­˜å‚¨
            img_buffer = io.BytesIO()
            image.save(img_buffer, format=format_name)
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            return {
                'text': image_description,
                'type': 'image',
                'metadata': {
                    'width': width,
                    'height': height,
                    'format': format_name,
                    'mode': mode,
                    'base64_data': img_base64,
                    'parser': 'PIL'
                },
                'summary': f"Image file: {file.filename} ({width}x{height})",
                'tags': ['image', format_name.lower()]
            }
            
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Image parsing failed: {str(e)}")
    
    async def parse_text_input(self, text: str, source: Optional[str], user_id: str) -> Dict[str, Any]:
        """è§£æçº¯æ–‡æœ¬è¾“å…¥å¹¶åˆ›å»ºè®°å¿†"""
        try:
            # ç”Ÿæˆembedding
            embedding = embedding_service.generate_embedding(
                text=text,
                input_type="passage"
            )
            
            # âœ… æ­£ç¡®çš„æ–¹å¼ - æŒ‰ç…§Linçš„ç‰ˆæœ¬
            memory_id = database_service.create_memory(
                content=text,              # ç¬¬1ä¸ªå‚æ•°
                memory_type='text',        # ç¬¬2ä¸ªå‚æ•°
                embedding=embedding,       # ç¬¬3ä¸ªå‚æ•°
                user_id=user_id,          # ç¬¬4ä¸ªå‚æ•° âœ…
                metadata={                 # ç¬¬5ä¸ªå‚æ•°ï¼ˆå‘½åå‚æ•°ï¼‰
                    'source': source or 'direct_input',
                    'parser': 'direct_text'
                },
                source=source,             # ç¬¬6ä¸ªå‚æ•°ï¼ˆå‘½åå‚æ•°ï¼‰
                summary=text[:200] + "..." if len(text) > 200 else text,  # ç¬¬7ä¸ªå‚æ•°
                tags=['text', 'direct_input']  # ç¬¬8ä¸ªå‚æ•° - ä½ å¯ä»¥ç”¨ [] æˆ– ['text', 'direct_input']
            )
            
            return {
                'success': True,
                'memory_id': memory_id,
                'content': text,
                'embedding_dimension': len(embedding)
            }
            
        except Exception as e:
            print(f"âŒ Text parsing failed: {e}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Text parsing failed: {str(e)}")
        
    def get_supported_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
        return list(self.supported_types.keys())
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥ä¾èµ–åº“
            import PyPDF2
            import docx
            from PIL import Image
            
            return {
                'status': 'healthy',
                'supported_types': self.get_supported_types(),
                'dependencies': {
                    'PyPDF2': 'available',
                    'python-docx': 'available',
                    'PIL': 'available'
                }
            }
            
        except ImportError as e:
            return {
                'status': 'unhealthy',
                'error': f"Missing dependency: {str(e)}",
                'supported_types': []
            }

# å…¨å±€å®ä¾‹
parser_service = ParserService()
