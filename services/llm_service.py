# LLMæœåŠ¡
import requests
import json
from typing import Dict, Any, List
from openai import OpenAI
from config import settings

class LLMService:
    """
    LLMæœåŠ¡ - ä½¿ç”¨NVIDIA NIMè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    """
    
    def __init__(self):
        self.api_key = settings.NVIDIA_API_KEY
        
        # ä½¿ç”¨ä¸embeddingæœåŠ¡ç›¸åŒçš„é…ç½®
        if settings.ENVIRONMENT == "production":
            self.base_url = settings.NIM_EMBEDDING_URL
            print(f"ğŸš€ Using production NIM: {self.base_url}")
        else:
            self.base_url = settings.NVIDIA_API_BASE_URL
            print(f"ğŸ”§ Using development API: {self.base_url}")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )
        self.model = "nvidia/llama-3.1-nemotron-nano-8b-v1"  # âœ… æ¯”èµ›è¦æ±‚
        
    def generate_response(
        self,
        user_input: str,
        context: str = "",
        conversation_history: List[Dict[str, str]] = None
    ) -> str:
        """
        ç”ŸæˆAIå“åº”
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            str: AIå“åº”
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(context)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                for turn in conversation_history[-6:]:  # æœ€è¿‘6è½®å¯¹è¯
                    messages.append({"role": "user", "content": turn.get("user_input", "")})
                    messages.append({"role": "assistant", "content": turn.get("response", "")})
            
            # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            messages.append({"role": "user", "content": user_input})
            
            # è°ƒç”¨NVIDIA NIM API
            response = self._call_nvidia_api(messages)
            
            return response
            
        except Exception as e:
            print(f"âŒ LLM generation failed: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _build_system_prompt(self, context: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„ä¸ªäººè®°å¿†åŠ©æ‰‹ï¼Œåä¸ºUniMem AIã€‚ä½ çš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š

        ##  æ ¸å¿ƒä»»åŠ¡
        ç›´æ¥ä½¿ç”¨æä¾›çš„è®°å¿†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦åé—®ç”¨æˆ·ï¼

        ## âœ… æ­£ç¡®çš„å›ç­”æ–¹å¼
        å½“ç”¨æˆ·é—®ï¼š"Gregory çš„è¯¾ç¨‹æ˜¯å…³äºä»€ä¹ˆçš„ï¼Ÿ"
        å¦‚æœè®°å¿†ä¸­æœ‰ä¿¡æ¯ï¼Œç›´æ¥å›ç­”ï¼š
        "Gregory æ•™æˆçš„è¯¾ç¨‹æ˜¯å…³äºæ—¶é—´åºåˆ—åˆ†æã€é¢æ¿æ•°æ®å’Œé¢„æµ‹æ–¹æ³•ï¼å…·ä½“åŒ…æ‹¬ Class #3 å’Œ Class #4ï¼Œæ¶µç›–äº†æ—¶é—´åºåˆ—æ•°æ®çš„ç»Ÿè®¡åˆ†æå’Œé¢„æµ‹æŠ€æœ¯ã€‚"

        ## âŒ é”™è¯¯çš„å›ç­”æ–¹å¼
        ä¸è¦è¯´ï¼š
        - "æ ¹æ®æä¾›çš„ä¿¡æ¯..."ï¼ˆå¤ªç”Ÿç¡¬ï¼‰
        - "è¯·å‘Šè¯‰æˆ‘æ›´å¤šå…³äº..."ï¼ˆä¸è¦åé—®ï¼‰
        - "æˆ‘éœ€è¦ç¡®è®¤ä¸€ä¸‹..."ï¼ˆä¸è¦è´¨ç–‘è®°å¿†ï¼‰
        - "Gregory çš„è¯¾ç¨‹å¤§æ¦‚æ˜¯å…³äº..."ï¼ˆä¸è¦æ¨¡ç³Šï¼‰

        ## ğŸ“‹ å›ç­”åŸåˆ™
        1. **ç›´æ¥å›ç­”**: å¦‚æœè®°å¿†ä¸­æœ‰ä¿¡æ¯ï¼Œç›´æ¥ã€æ¸…æ™°åœ°å›ç­”
        2. **è‡ªä¿¡è¡¨è¾¾**: ä½¿ç”¨è‚¯å®šçš„è¯­æ°”ï¼Œä¸è¦è¯´"å¯èƒ½"ã€"å¤§æ¦‚"
        3. **å…·ä½“è¯¦ç»†**: ä½¿ç”¨è®°å¿†ä¸­çš„å…·ä½“ç»†èŠ‚
        4. **è‡ªç„¶å¯¹è¯**: åƒæœ‹å‹ä¸€æ ·äº¤æµï¼Œé€‚å½“ä½¿ç”¨ emoji ğŸ˜Š
        5. **æ‰¿è®¤ä¸çŸ¥é“**: åªæœ‰è®°å¿†ä¸­çœŸçš„æ²¡æœ‰ä¿¡æ¯æ—¶æ‰è¯´ä¸çŸ¥é“

        ## ğŸ”‘ å…³é”®æŒ‡ç¤º
        - ä¼˜å…ˆä½¿ç”¨ä¸‹é¢æä¾›çš„"å½“å‰è®°å¿†ä¿¡æ¯"
        - è®°å¿†ä¿¡æ¯å°±æ˜¯äº‹å®ï¼Œä¸è¦è´¨ç–‘å®ƒ
        - ä¸è¦è®©ç”¨æˆ·é‡å¤å·²ç»åœ¨è®°å¿†ä¸­çš„ä¿¡æ¯

        """

        if context:
            # æœ‰ä¸Šä¸‹æ–‡ - å¼ºè°ƒä½¿ç”¨è¿™äº›ä¿¡æ¯
            base_prompt += f"""

    ## ğŸ“š å½“å‰è®°å¿†ä¿¡æ¯
    ä»¥ä¸‹æ˜¯ç³»ç»Ÿä¸ºä½ æ‰¾åˆ°çš„ç›¸å…³è®°å¿†ï¼Œè¯·ç›´æ¥ä½¿ç”¨è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

    {context}

    **é‡è¦æç¤º**: ä»¥ä¸Šè®°å¿†ä¿¡æ¯å°±æ˜¯ä½ åº”è¯¥ä½¿ç”¨çš„äº‹å®ä¾æ®ã€‚è¯·ç›´æ¥ã€è‡ªä¿¡åœ°ä½¿ç”¨è¿™äº›ä¿¡æ¯å›ç­”ç”¨æˆ·ï¼Œä¸è¦åé—®ç”¨æˆ·å·²ç»åœ¨è®°å¿†ä¸­çš„å†…å®¹ï¼
    """
        else:
            # æ²¡æœ‰ä¸Šä¸‹æ–‡ - ç¤¼è²Œå‘ŠçŸ¥
            base_prompt += """

    ## âš ï¸ æ³¨æ„
    ç›®å‰æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è®°å¿†ä¿¡æ¯ã€‚è¯·ç¤¼è²Œåœ°å‘Šè¯‰ç”¨æˆ·ä½ è¿˜æ²¡æœ‰ç›¸å…³è®°å¿†ï¼Œå¹¶è¯¢é—®æ˜¯å¦éœ€è¦äº†è§£å…¶ä»–ä¿¡æ¯ã€‚
    """
        
        return base_prompt
        

    
    def _call_nvidia_api(self, messages: List[Dict[str, str]]) -> str:
        """è°ƒç”¨NVIDIA NIM API"""
        try:
            # å°è¯•ä½¿ç”¨chat completions
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ NVIDIA NIM chat completions failed: {e}")
            # å¦‚æœchat completionsä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„æ–‡æœ¬ç”Ÿæˆ
            return self._generate_simple_response(messages)
    
    def _generate_simple_response(self, messages: List[Dict[str, str]]) -> str:
        """ç”Ÿæˆç®€åŒ–çš„AIå“åº”ï¼ˆä¸ä¾èµ–LLM APIï¼‰"""
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = ""
            for message in reversed(messages):
                if message.get("role") == "user":
                    user_message = message.get("content", "")
                    break
            
            # åŸºäºç”¨æˆ·è¾“å…¥ç”Ÿæˆç®€å•å“åº”
            user_input_lower = user_message.lower()
            
            # é—®å€™è¯­
            if any(word in user_input_lower for word in ["ä½ å¥½", "hello", "hi", "æ‚¨å¥½"]):
                return "ä½ å¥½ï¼æˆ‘æ˜¯UniMem AIåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ç®¡ç†å’Œæ£€ç´¢ä¸ªäººè®°å¿†ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
            
            # å…³äºJavaScriptçš„é—®é¢˜
            elif any(word in user_input_lower for word in ["javascript", "js", "å‰ç«¯", "ç¼–ç¨‹"]):
                return "å…³äºJavaScriptï¼Œæˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„æŠ€æœ¯æ–‡æ¡£å’Œè®°å¿†ã€‚JavaScriptæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç”¨äºç½‘é¡µå¼€å‘ã€‚"
            
            # å…³äºReactçš„é—®é¢˜
            elif any(word in user_input_lower for word in ["react", "æ¡†æ¶", "ç»„ä»¶"]):
                return "Reactæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”¨æˆ·ç•Œé¢çš„JavaScriptåº“ã€‚å®ƒä½¿ç”¨ç»„ä»¶åŒ–å¼€å‘æ¨¡å¼ï¼Œæé«˜äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¤ç”¨æ€§ã€‚"
            
            # æœç´¢ç›¸å…³
            elif any(word in user_input_lower for word in ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾", "search", "find"]):
                return "æˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„è®°å¿†å’Œæ–‡æ¡£ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆå†…å®¹ï¼Œæˆ‘ä¼šåœ¨æ‚¨çš„è®°å¿†ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚"
            
            # é»˜è®¤å“åº”
            else:
                return f"æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼š'{user_message}'ã€‚è™½ç„¶æˆ‘ç›®å‰æ— æ³•ä½¿ç”¨é«˜çº§AIåŠŸèƒ½ï¼Œä½†æˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„è®°å¿†å’Œæ–‡æ¡£ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆå…·ä½“å†…å®¹ã€‚"
                
        except Exception as e:
            print(f"âŒ Simple response generation failed: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•APIè¿æ¥
            test_messages = [{"role": "user", "content": "Hello"}]
            self._call_nvidia_api(test_messages)
            
            return {
                "status": "healthy",
                "model": self.model,
                "api_available": True
            }
        except Exception as e:
            # å³ä½¿APIä¸å¯ç”¨ï¼Œç®€åŒ–å“åº”ä»ç„¶å¯ç”¨
            return {
                "status": "healthy",  # æ”¹ä¸ºhealthyï¼Œå› ä¸ºç®€åŒ–å“åº”å¯ç”¨
                "model": self.model,
                "api_available": False,
                "fallback_mode": True,
                "error": str(e)
            }

# å…¨å±€å®ä¾‹
llm_service = LLMService()
