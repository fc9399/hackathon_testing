# LLMæœåŠ¡
import requests
import json
from typing import Dict, Any, List
from openai import OpenAI
from config import settings

class LLMService:
    """
    LLMæœåŠ¡ - ä½¿ç”¨NVIDIA NIMè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    """
    
    def __init__(self):
        self.api_key = settings.NVIDIA_API_KEY
        
        # ä½¿ç”¨ä¸embeddingæœåŠ¡ç›¸åŒçš„é…ç½®
        if settings.ENVIRONMENT == "production":
            self.base_url = settings.NIM_EMBEDDING_URL
            print(f"ğŸš€ Using production NIM: {self.base_url}")
        else:
            self.base_url = settings.NVIDIA_API_BASE_URL
            print(f"ğŸ”§ Using development API: {self.base_url}")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )
        self.model = "meta/llama-3.2-11b-instruct"  # ä½¿ç”¨11Bæ¨¡å‹ï¼Œæ›´ç¨³å®š
        
    def generate_response(
        self,
        user_input: str,
        context: str = "",
        conversation_history: List[Dict[str, str]] = None
    ) -> str:
        """
        ç”ŸæˆAIå“åº”
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            str: AIå“åº”
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(context)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                for turn in conversation_history[-6:]:  # æœ€è¿‘6è½®å¯¹è¯
                    messages.append({"role": "user", "content": turn.get("user_input", "")})
                    messages.append({"role": "assistant", "content": turn.get("response", "")})
            
            # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
            messages.append({"role": "user", "content": user_input})
            
            # è°ƒç”¨NVIDIA NIM API
            response = self._call_nvidia_api(messages)
            
            return response
            
        except Exception as e:
            print(f"âŒ LLM generation failed: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _build_system_prompt(self, context: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„ä¸ªäººè®°å¿†åŠ©æ‰‹ï¼Œåä¸ºUniMem AIã€‚ä½ çš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š

1. å¸®åŠ©ç”¨æˆ·ç®¡ç†å’Œæ£€ç´¢ä¸ªäººè®°å¿†
2. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜
3. è¿›è¡Œè‡ªç„¶ã€å‹å¥½çš„å¯¹è¯
4. è®°ä½ç”¨æˆ·çš„é‡è¦ä¿¡æ¯

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
- å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ã€å‹å¥½
- å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›ä¿¡æ¯
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®åœ°è¯´ä¸çŸ¥é“
- ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…
- é€‚å½“ä½¿ç”¨emojiè®©å¯¹è¯æ›´ç”ŸåŠ¨

"""
        
        if context:
            base_prompt += f"\nå½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{context}\n"
        
        return base_prompt
    
    def _call_nvidia_api(self, messages: List[Dict[str, str]]) -> str:
        """è°ƒç”¨NVIDIA NIM API"""
        try:
            # å°è¯•ä½¿ç”¨chat completions
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ NVIDIA NIM chat completions failed: {e}")
            # å¦‚æœchat completionsä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„æ–‡æœ¬ç”Ÿæˆ
            return self._generate_simple_response(messages)
    
    def _generate_simple_response(self, messages: List[Dict[str, str]]) -> str:
        """ç”Ÿæˆç®€åŒ–çš„AIå“åº”ï¼ˆä¸ä¾èµ–LLM APIï¼‰"""
        try:
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = ""
            for message in reversed(messages):
                if message.get("role") == "user":
                    user_message = message.get("content", "")
                    break
            
            # åŸºäºç”¨æˆ·è¾“å…¥ç”Ÿæˆç®€å•å“åº”
            user_input_lower = user_message.lower()
            
            # é—®å€™è¯­
            if any(word in user_input_lower for word in ["ä½ å¥½", "hello", "hi", "æ‚¨å¥½"]):
                return "ä½ å¥½ï¼æˆ‘æ˜¯UniMem AIåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ç®¡ç†å’Œæ£€ç´¢ä¸ªäººè®°å¿†ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
            
            # å…³äºJavaScriptçš„é—®é¢˜
            elif any(word in user_input_lower for word in ["javascript", "js", "å‰ç«¯", "ç¼–ç¨‹"]):
                return "å…³äºJavaScriptï¼Œæˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„æŠ€æœ¯æ–‡æ¡£å’Œè®°å¿†ã€‚JavaScriptæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç”¨äºç½‘é¡µå¼€å‘ã€‚"
            
            # å…³äºReactçš„é—®é¢˜
            elif any(word in user_input_lower for word in ["react", "æ¡†æ¶", "ç»„ä»¶"]):
                return "Reactæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”¨æˆ·ç•Œé¢çš„JavaScriptåº“ã€‚å®ƒä½¿ç”¨ç»„ä»¶åŒ–å¼€å‘æ¨¡å¼ï¼Œæé«˜äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¤ç”¨æ€§ã€‚"
            
            # æœç´¢ç›¸å…³
            elif any(word in user_input_lower for word in ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾", "search", "find"]):
                return "æˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„è®°å¿†å’Œæ–‡æ¡£ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆå†…å®¹ï¼Œæˆ‘ä¼šåœ¨æ‚¨çš„è®°å¿†ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚"
            
            # é»˜è®¤å“åº”
            else:
                return f"æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼š'{user_message}'ã€‚è™½ç„¶æˆ‘ç›®å‰æ— æ³•ä½¿ç”¨é«˜çº§AIåŠŸèƒ½ï¼Œä½†æˆ‘å¯ä»¥å¸®æ‚¨æœç´¢ç›¸å…³çš„è®°å¿†å’Œæ–‡æ¡£ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆå…·ä½“å†…å®¹ã€‚"
                
        except Exception as e:
            print(f"âŒ Simple response generation failed: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•APIè¿æ¥
            test_messages = [{"role": "user", "content": "Hello"}]
            self._call_nvidia_api(test_messages)
            
            return {
                "status": "healthy",
                "model": self.model,
                "api_available": True
            }
        except Exception as e:
            # å³ä½¿APIä¸å¯ç”¨ï¼Œç®€åŒ–å“åº”ä»ç„¶å¯ç”¨
            return {
                "status": "healthy",  # æ”¹ä¸ºhealthyï¼Œå› ä¸ºç®€åŒ–å“åº”å¯ç”¨
                "model": self.model,
                "api_available": False,
                "fallback_mode": True,
                "error": str(e)
            }

# å…¨å±€å®ä¾‹
llm_service = LLMService()
